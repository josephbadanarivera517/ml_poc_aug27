{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jPl-T9H0ALO",
        "outputId": "ce82c191-122d-4f22-cdcd-3a56fc6ee20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 256, 32)           320000    \n",
            "                                                                 \n",
            " global_average_pooling1d_8  (None, 32)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321089 (1.22 MB)\n",
            "Trainable params: 321089 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "782/782 - 7s - loss: 0.4471 - accuracy: 0.8054 - val_loss: 0.3108 - val_accuracy: 0.8682 - 7s/epoch - 9ms/step\n",
            "Epoch 2/10\n",
            "782/782 - 5s - loss: 0.2355 - accuracy: 0.9106 - val_loss: 0.2900 - val_accuracy: 0.8806 - 5s/epoch - 6ms/step\n",
            "Epoch 3/10\n",
            "782/782 - 6s - loss: 0.1857 - accuracy: 0.9312 - val_loss: 0.2923 - val_accuracy: 0.8824 - 6s/epoch - 7ms/step\n",
            "Epoch 4/10\n",
            "782/782 - 5s - loss: 0.1521 - accuracy: 0.9444 - val_loss: 0.3219 - val_accuracy: 0.8742 - 5s/epoch - 7ms/step\n",
            "Epoch 5/10\n",
            "782/782 - 6s - loss: 0.1287 - accuracy: 0.9555 - val_loss: 0.3493 - val_accuracy: 0.8707 - 6s/epoch - 7ms/step\n",
            "Epoch 6/10\n",
            "782/782 - 5s - loss: 0.1110 - accuracy: 0.9624 - val_loss: 0.3976 - val_accuracy: 0.8610 - 5s/epoch - 7ms/step\n",
            "Epoch 7/10\n",
            "782/782 - 6s - loss: 0.0962 - accuracy: 0.9682 - val_loss: 0.4291 - val_accuracy: 0.8622 - 6s/epoch - 7ms/step\n",
            "Epoch 8/10\n",
            "782/782 - 5s - loss: 0.0843 - accuracy: 0.9731 - val_loss: 0.4687 - val_accuracy: 0.8568 - 5s/epoch - 7ms/step\n",
            "Epoch 9/10\n",
            "782/782 - 5s - loss: 0.0718 - accuracy: 0.9792 - val_loss: 0.5149 - val_accuracy: 0.8559 - 5s/epoch - 6ms/step\n",
            "Epoch 10/10\n",
            "782/782 - 6s - loss: 0.0624 - accuracy: 0.9822 - val_loss: 0.5624 - val_accuracy: 0.8489 - 6s/epoch - 8ms/step\n",
            "782/782 - 1s - loss: 0.5624 - accuracy: 0.8489 - 1s/epoch - 2ms/step\n",
            "Test Accuracy: 0.8488799929618835, Test Loss: 0.5623828768730164\n",
            "782/782 [==============================] - 2s 3ms/step\n",
            "Incorrect Prediction 1:\n",
            "Review: i generally love this type of movie however this time i found myself wanting to kick the screen since i can't do that i will just complain about it this was absolutely idiotic the things that happen with the dead kids are very cool but the alive people are absolute idiots i am a grown man pretty big and i can defend myself well however i would not do half the stuff the little girl does in this movie also the mother in this movie is reckless with her children to the point of neglect i wish i wasn't so angry about her and her actions because i would have otherwise enjoyed the flick what a number she was take my advise and fast forward through everything you see her do until the end also is anyone else getting sick of watching movies that are filmed so dark anymore one can hardly see what is being filmed as an audience we are involved with the actions on the screen so then why the hell can't we have night vision\n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Incorrect Prediction 2:\n",
            "Review: hollywood had a long love affair with bogus nights tales but few of these products have stood the test of time the most memorable were the jon hall maria films which have long since become camp this one is filled with dubbed songs and slapstick it's a truly crop of corn and pretty near today it was nominated for its imaginative special effects which are almost in this day and age mainly of trick photography the only outstanding positive feature which survives is its beautiful color and clarity sad to say of the many films made in this genre few of them come up to alexander original thief of almost any other nights film is superior to this one though it's a loser\n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Incorrect Prediction 3:\n",
            "Review: ed mitchell is a teenager who lives for his job at good a small but friendly neighborhood stand while his buddy thompson also works there but lack single minded devotion to his job he's there because he accidentally destroyed the car of his teacher mr and has to raise money to pay the when a fast foot chain opens across the street it looks like good is history until ed a secret that brings hundreds of new customers to their door however the manager of kurt jan is determined to get his hands on the and put good out of business meanwhile ed and must rescue the world's oldest fast food employee from the demented hills asylum and ed might just find love with jackson if he could take his mind off the long enough to pay attention to her good is a comedy directed for kids decent story acting and overall a pretty harmless kids movie\n",
            "Actual Sentiment: Negative\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Review 1: This movie was an excellent portrayal of character development and had stellar acting.\n",
            "Predicted Score: [0.7859514]\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 2: I found the movie to be predictable with a lackluster script.\n",
            "Predicted Score: [0.4686651]\n",
            "Predicted Sentiment: Negative\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 3: The cinematography was magnificent, and the pacing was perfect. Highly recommend watching.\n",
            "Predicted Score: [0.10423147]\n",
            "Predicted Sentiment: Negative\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 4: It was a terrible movie that wasted two hours of my life. The plot made no sense.\n",
            "Predicted Score: [0.6756865]\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 5: An absolute masterpiece, with a gripping story and profound performances.\n",
            "Predicted Score: [0.9097953]\n",
            "Predicted Sentiment: Positive\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "# Constants for data preprocessing\n",
        "max_length = 256  # Maximum length of the sequences\n",
        "padding_type = 'post'  # Padding type for sequences shorter than the maximum length\n",
        "vocab_size = 10000  # Size of the vocabulary used in the Embedding layer\n",
        "embedding_dim=16  # Dimensionality of the embedding layer\n",
        "hidden_units=16  # Number of hidden units in the LSTM layer\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Helper function to preprocess data\n",
        "def preprocess_data(data):\n",
        "    return pad_sequences(data, maxlen=max_length, padding=padding_type)\n",
        "\n",
        "# Preprocess the data\n",
        "train_data = preprocess_data(train_data)\n",
        "test_data = preprocess_data(test_data)\n",
        "\n",
        "# Define the model architecture\n",
        "def build_model(vocab_size, embedding_dim, hidden_units):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(hidden_units, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_model(vocab_size, embedding_dim, hidden_units)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train and evaluate the model\n",
        "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels), verbose=2)\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")\n",
        "\n",
        "# Decode review function\n",
        "word_index = imdb.get_word_index()\n",
        "def decode_review(encoded_review):\n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i >= 3])\n",
        "\n",
        "# Display incorrect predictions\n",
        "def display_incorrect_predictions(test_data, test_labels, predictions, num_examples=3):\n",
        "    predicted_classes = (predictions > 0.5).astype(int)\n",
        "    incorrect_indices = np.where(predicted_classes.flatten() != test_labels)[0]\n",
        "    for i, idx in enumerate(incorrect_indices[:num_examples]):\n",
        "        print(f\"Incorrect Prediction {i+1}:\")\n",
        "        print(f\"Review: {decode_review(test_data[idx])}\")\n",
        "        print(f\"Actual Sentiment: {'Positive' if test_labels[idx] == 1 else 'Negative'}\")\n",
        "        print(f\"Predicted Sentiment: {'Positive' if predicted_classes[idx][0] == 1 else 'Negative'}\")\n",
        "        print(\"--------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "predictions = model.predict(test_data)\n",
        "display_incorrect_predictions(test_data, test_labels, predictions)\n",
        "\n",
        "# Predict sentiments for sample reviews and display them\n",
        "def predict_and_display_reviews(reviews):\n",
        "    sequences = [[word_index.get(word, 2) for word in review.lower().split()] for review in reviews]\n",
        "    padded_sequences = preprocess_data(sequences)\n",
        "    sample_predictions = model.predict(padded_sequences)\n",
        "    sample_predicted_classes = (sample_predictions > 0.5).astype(int)\n",
        "    for i, review in enumerate(reviews):\n",
        "        print(f\"Review {i+1}: {review}\")\n",
        "        print(f'Predicted Score: {sample_predictions[i]}')\n",
        "        print(f\"Predicted Sentiment: {'Positive' if sample_predicted_classes[i][0] == 1 else 'Negative'}\")\n",
        "        print(\"--------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "# Sample movie reviews\n",
        "reviews = [\n",
        "    \"This movie was an excellent portrayal of character development and had stellar acting.\",\n",
        "    \"I found the movie to be predictable with a lackluster script.\",\n",
        "    \"The cinematography was magnificent, and the pacing was perfect. Highly recommend watching.\",\n",
        "    \"It was a terrible movie that wasted two hours of my life. The plot made no sense.\",\n",
        "    \"An absolute masterpiece, with a gripping story and profound performances.\"\n",
        "]\n",
        "\n",
        "predict_and_display_reviews(reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu2TzqcK_B1y",
        "outputId": "0ad61e16-dd16-4d12-b916-db9fee0c2f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 256, 16)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256, 64)           20736     \n",
            "                                                                 \n",
            " attention (Attention)       ((None, 64),              8385      \n",
            "                              (None, 256, 1))                    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 193346 (755.26 KB)\n",
            "Trainable params: 193346 (755.26 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 - 152s - loss: 0.5101 - accuracy: 0.7348 - val_loss: 0.3242 - val_accuracy: 0.8610 - 152s/epoch - 195ms/step\n",
            "Epoch 2/5\n",
            "782/782 - 148s - loss: 0.2471 - accuracy: 0.9032 - val_loss: 0.3134 - val_accuracy: 0.8669 - 148s/epoch - 190ms/step\n",
            "Epoch 3/5\n",
            "782/782 - 148s - loss: 0.1847 - accuracy: 0.9311 - val_loss: 0.3029 - val_accuracy: 0.8716 - 148s/epoch - 189ms/step\n",
            "Epoch 4/5\n",
            "782/782 - 159s - loss: 0.1512 - accuracy: 0.9475 - val_loss: 0.3858 - val_accuracy: 0.8574 - 159s/epoch - 203ms/step\n",
            "Epoch 5/5\n",
            "782/782 - 148s - loss: 0.1140 - accuracy: 0.9623 - val_loss: 0.4106 - val_accuracy: 0.8515 - 148s/epoch - 189ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x787f1f0b2d10>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 256\n",
        "embedding_dim = 16\n",
        "lstm_units = 64\n",
        "\n",
        "# Load IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Preprocess data: Pad sequences to ensure uniform input size\n",
        "train_data = pad_sequences(train_data, maxlen=max_length, padding='post')\n",
        "test_data = pad_sequences(test_data, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define a simple Attention Layer\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "        self.U = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: [batch_size, time_steps, input_dim]\n",
        "        score = tf.nn.tanh(self.W(inputs) + self.U(inputs))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Build the model\n",
        "input_layer = Input(shape=(max_length,))\n",
        "embed = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
        "lstm = LSTM(lstm_units, return_sequences=True)(embed)\n",
        "attention_output, attention_weights = Attention(lstm_units)(lstm)\n",
        "dense = Dense(64, activation='relu')(attention_output)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=5, batch_size=32, validation_data=(test_data, test_labels), verbose=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm6kdDzDstl1"
      },
      "source": [
        "# Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "Ol9Rd8cFq9X_",
        "outputId": "4bc95b0e-cb2c-4e52-bbde-f45a8d66adf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 200, 2) are incompatible\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a45d6ef3aa1b>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 200, 2) are incompatible\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Custom Dot-Product Attention Layer\n",
        "class SimpleAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SimpleAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = self.add_weight(name='attention_weight',\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        super(SimpleAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W))\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 5000\n",
        "embedding_dim = 32\n",
        "max_length = 200\n",
        "lstm_units = 64\n",
        "\n",
        "# Load IMDB Dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Preprocessing\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_length)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build the Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
        "model.add(SimpleAttention())\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "print(cm)\n",
        "\n",
        "# Function to decode the reviews\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "\n",
        "# Incorrect Predictions Examples\n",
        "incorrect_indices = np.nonzero(y_pred_classes != y_true)[0]\n",
        "for i in incorrect_indices[:5]:\n",
        "    text = X_test[i]\n",
        "    decoded_text = decode_review(text)\n",
        "    print('Original Review:', decoded_text)\n",
        "    print('Predicted Sentiment:', 'Positive' if y_pred_classes[i] == 1 else 'Negative')\n",
        "    print('True Sentiment:', 'Positive' if y_true[i] == 1 else 'Negative')\n",
        "    print('---')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
